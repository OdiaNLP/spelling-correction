{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import List, Union\n",
    "\n",
    "from gensim.models import FastText\n",
    "from indicnlp.tokenize.indic_tokenize import trivial_tokenize_indic\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# set up the logging to monitor gensim\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s - %(asctime)s: %(message)s\",\n",
    "    datefmt='%H:%M:%S',\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def tokenize_text(text: List[str]) -> List[List[str]]:\n",
    "    \"\"\"Tokenize text\"\"\"\n",
    "    return [trivial_tokenize_indic(sent) for sent in tqdm(text, desc='tokenize', unit=' sentences')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train_fasttext(tokenized_text: List[List[str]], size: int = 100, window: int = 5, min_count: int = 1,\n",
    "                   epochs: int = 10,\n",
    "                   random_seed: int = 123, vec_file_path: Union[str, None] = None, ):\n",
    "    \"\"\"Learn fasttext embeddings\"\"\"\n",
    "    # count the number of cores\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    # create fasttext model\n",
    "    model = FastText(\n",
    "        size=size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=cores - 1,\n",
    "        seed=random_seed,\n",
    "    )\n",
    "    # build vocab\n",
    "    model.build_vocab(sentences=tokenized_text, progress_per=1000000)  # show progress after processing every 1M words\n",
    "    # train\n",
    "    model.train(sentences=tokenized_text, total_examples=model.corpus_count, epochs=epochs,\n",
    "                report_delay=10)  # show progress after every 10 seconds\n",
    "    if vec_file_path is not None:\n",
    "        model.save(vec_file_path)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data\n",
    "For learning the Odia word embeddings, we need monolingual Odia text data. You can possibly scrape data from an online source such as Wikipedia. For our experiments now, let's take the Odia monolingual text data available as part of the Indic NLP corpus."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "filename = os.path.join('data/or')\n",
    "assert os.path.isfile(filename)  # sanity check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read lines from file: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3594672/3594672 [00:02<00:00, 1405592.12it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    lines = [s.strip() for s in tqdm(f.readlines(), desc='read lines from file')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3594672/3594672 [01:33<00:00, 38276.08 sentences/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tokens = tokenize_text(lines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute frequencies of tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3594672/3594672 [00:26<00:00, 135718.34 sentences/s]\n"
     ]
    }
   ],
   "source": [
    "num_running_toks, num_unique_toks = 0, 0\n",
    "counter = Counter()\n",
    "for toks in tqdm(tokens, desc='compute frequencies of tokens', unit=' sentences'):\n",
    "    counter.update(toks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3,594,672\n",
      "Number of unique words or equivalantly, the size of vocabulary: 778,862\n",
      "Number of running words: 51,151,273\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of sentences: {len(lines):,}')\n",
    "print(f'Number of unique words or equivalantly, the size of vocabulary: {len(counter):,}')\n",
    "print(f'Number of running words: {sum([freq for _, freq in counter.items()]):,}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[('à¥¤', 3393061),\n (',', 1191253),\n ('à¬“', 534792),\n ('à¬à¬¹à¬¿', 437185),\n ('à¬ªà¬¾à¬‡à¬', 373726),\n ('à¬¸à­‡', 240775),\n ('à¬¬à­‹à¬²à¬¿', 239837),\n ('à¬ªà¬°à­‡', 224959),\n ('à¬•à¬°à¬¿', 221628),\n ('à¬à¬•', 213516),\n ('à¬®à¬§à­à­Ÿ', 210907),\n ('à¬à¬¬à¬‚', 198988),\n ('à¬•à¬°à¬¿à¬¥à¬¿à¬²à­‡', 195168),\n ('à¬¸à¬¹', 177040),\n ('-', 174796),\n ('à¬–à¬¬à¬°', 169373),\n ('.', 166728),\n ('à¬•à¬°à¬¿à¬¬à¬¾', 166276),\n ('à¬¨à­‡à¬‡', 161728),\n ('à¬¬à­‡à¬³à­‡', 156327)]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# most common words\n",
    "counter.most_common(n=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learn embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:36:14: resetting layer weights\n",
      "INFO - 15:36:28: collecting all words and their counts\n",
      "INFO - 15:36:28: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 15:36:35: PROGRESS: at sentence #1000000, processed 14406915 words, keeping 356423 word types\n",
      "INFO - 15:36:40: PROGRESS: at sentence #2000000, processed 28227997 words, keeping 518060 word types\n",
      "INFO - 15:36:45: PROGRESS: at sentence #3000000, processed 42532970 words, keeping 692676 word types\n",
      "INFO - 15:36:48: collected 778862 word types from a corpus of 51151273 raw words and 3594672 sentences\n",
      "INFO - 15:36:48: Loading a fresh vocabulary\n",
      "INFO - 15:36:49: effective_min_count=20 retains 72827 unique words (9% of original 778862, drops 706035)\n",
      "INFO - 15:36:49: effective_min_count=20 leaves 49262024 word corpus (96% of original 51151273, drops 1889249)\n",
      "INFO - 15:36:49: deleting the raw counts dictionary of 778862 items\n",
      "INFO - 15:36:49: sample=0.001 downsamples 22 most-common words\n",
      "INFO - 15:36:49: downsampling leaves estimated 43948889 word corpus (89.2% of prior 49262024)\n",
      "INFO - 15:36:51: estimated required memory for 72827 words, 333945 buckets and 100 dimensions: 245357740 bytes\n",
      "INFO - 15:36:51: resetting layer weights\n",
      "INFO - 15:37:13: training model with 3 workers on 72827 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 15:37:15: EPOCH 1 - PROGRESS: at 0.53% examples, 235989 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:37:25: EPOCH 1 - PROGRESS: at 5.77% examples, 247401 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:37:35: EPOCH 1 - PROGRESS: at 11.50% examples, 248815 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 15:37:45: EPOCH 1 - PROGRESS: at 17.15% examples, 248197 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:37:55: EPOCH 1 - PROGRESS: at 22.89% examples, 249518 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:38:05: EPOCH 1 - PROGRESS: at 28.90% examples, 252710 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:38:15: EPOCH 1 - PROGRESS: at 35.12% examples, 255982 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:38:25: EPOCH 1 - PROGRESS: at 40.89% examples, 255547 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:38:35: EPOCH 1 - PROGRESS: at 47.12% examples, 257650 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:38:45: EPOCH 1 - PROGRESS: at 54.13% examples, 260999 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:38:55: EPOCH 1 - PROGRESS: at 61.46% examples, 262990 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:39:05: EPOCH 1 - PROGRESS: at 69.04% examples, 265457 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:39:15: EPOCH 1 - PROGRESS: at 75.60% examples, 266901 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:39:25: EPOCH 1 - PROGRESS: at 80.40% examples, 266698 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:39:35: EPOCH 1 - PROGRESS: at 86.17% examples, 267524 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:39:45: EPOCH 1 - PROGRESS: at 92.08% examples, 268162 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:39:55: EPOCH 1 - PROGRESS: at 98.59% examples, 268911 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:39:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 15:39:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 15:39:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 15:39:57: EPOCH - 1 : training on 51151273 raw words (43948186 effective words) took 163.3s, 269200 effective words/s\n",
      "INFO - 15:39:58: EPOCH 2 - PROGRESS: at 0.60% examples, 271473 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:40:08: EPOCH 2 - PROGRESS: at 6.46% examples, 275256 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:40:18: EPOCH 2 - PROGRESS: at 12.92% examples, 279437 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:40:28: EPOCH 2 - PROGRESS: at 18.40% examples, 266365 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:40:38: EPOCH 2 - PROGRESS: at 24.46% examples, 266592 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:40:48: EPOCH 2 - PROGRESS: at 30.20% examples, 264062 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:40:58: EPOCH 2 - PROGRESS: at 36.26% examples, 264288 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:41:08: EPOCH 2 - PROGRESS: at 42.68% examples, 266496 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:41:18: EPOCH 2 - PROGRESS: at 48.84% examples, 266708 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:41:28: EPOCH 2 - PROGRESS: at 55.71% examples, 267369 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:41:38: EPOCH 2 - PROGRESS: at 63.33% examples, 269756 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:41:48: EPOCH 2 - PROGRESS: at 70.87% examples, 271478 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:41:58: EPOCH 2 - PROGRESS: at 76.80% examples, 272211 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:42:08: EPOCH 2 - PROGRESS: at 81.45% examples, 271088 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:42:18: EPOCH 2 - PROGRESS: at 86.89% examples, 269965 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:42:28: EPOCH 2 - PROGRESS: at 92.88% examples, 270481 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:42:38: EPOCH 2 - PROGRESS: at 98.21% examples, 268036 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:42:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 15:42:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 15:42:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 15:42:42: EPOCH - 2 : training on 51151273 raw words (43948685 effective words) took 165.1s, 266269 effective words/s\n",
      "INFO - 15:42:43: EPOCH 3 - PROGRESS: at 0.42% examples, 182470 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:42:53: EPOCH 3 - PROGRESS: at 5.79% examples, 247705 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:43:03: EPOCH 3 - PROGRESS: at 12.43% examples, 268195 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:43:13: EPOCH 3 - PROGRESS: at 18.73% examples, 270481 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:43:23: EPOCH 3 - PROGRESS: at 24.63% examples, 267930 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:43:33: EPOCH 3 - PROGRESS: at 30.01% examples, 261919 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:43:43: EPOCH 3 - PROGRESS: at 35.61% examples, 259119 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:43:53: EPOCH 3 - PROGRESS: at 41.22% examples, 257275 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:44:03: EPOCH 3 - PROGRESS: at 47.43% examples, 258948 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:44:13: EPOCH 3 - PROGRESS: at 54.63% examples, 262715 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:44:23: EPOCH 3 - PROGRESS: at 60.97% examples, 260712 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:44:33: EPOCH 3 - PROGRESS: at 68.70% examples, 264003 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:44:43: EPOCH 3 - PROGRESS: at 75.20% examples, 264777 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:44:53: EPOCH 3 - PROGRESS: at 80.10% examples, 265315 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:45:03: EPOCH 3 - PROGRESS: at 85.21% examples, 264154 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:45:13: EPOCH 3 - PROGRESS: at 90.51% examples, 263626 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:45:23: EPOCH 3 - PROGRESS: at 96.27% examples, 262720 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:45:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 15:45:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 15:45:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 15:45:29: EPOCH - 3 : training on 51151273 raw words (43948073 effective words) took 167.5s, 262346 effective words/s\n",
      "INFO - 15:45:30: EPOCH 4 - PROGRESS: at 0.55% examples, 244599 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:45:40: EPOCH 4 - PROGRESS: at 6.20% examples, 265049 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:45:50: EPOCH 4 - PROGRESS: at 12.39% examples, 267809 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:46:00: EPOCH 4 - PROGRESS: at 18.14% examples, 262601 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:46:10: EPOCH 4 - PROGRESS: at 23.99% examples, 261509 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:46:20: EPOCH 4 - PROGRESS: at 29.95% examples, 261611 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:46:31: EPOCH 4 - PROGRESS: at 35.77% examples, 260492 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:46:41: EPOCH 4 - PROGRESS: at 41.54% examples, 259432 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:46:51: EPOCH 4 - PROGRESS: at 47.22% examples, 258100 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:47:01: EPOCH 4 - PROGRESS: at 53.93% examples, 260079 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:47:11: EPOCH 4 - PROGRESS: at 60.92% examples, 260809 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:47:21: EPOCH 4 - PROGRESS: at 67.72% examples, 260859 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:47:31: EPOCH 4 - PROGRESS: at 74.74% examples, 262895 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:47:41: EPOCH 4 - PROGRESS: at 79.64% examples, 263587 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:47:51: EPOCH 4 - PROGRESS: at 85.40% examples, 265094 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:48:01: EPOCH 4 - PROGRESS: at 91.34% examples, 266204 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:48:11: EPOCH 4 - PROGRESS: at 98.03% examples, 267686 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:48:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 15:48:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 15:48:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 15:48:13: EPOCH - 4 : training on 51151273 raw words (43949686 effective words) took 164.1s, 267874 effective words/s\n",
      "INFO - 15:48:14: EPOCH 5 - PROGRESS: at 0.60% examples, 271375 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:48:24: EPOCH 5 - PROGRESS: at 6.44% examples, 273848 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:48:35: EPOCH 5 - PROGRESS: at 13.08% examples, 281841 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:48:45: EPOCH 5 - PROGRESS: at 19.41% examples, 280113 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 15:48:55: EPOCH 5 - PROGRESS: at 26.03% examples, 282827 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:49:05: EPOCH 5 - PROGRESS: at 32.84% examples, 286220 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:49:15: EPOCH 5 - PROGRESS: at 39.51% examples, 287240 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:49:25: EPOCH 5 - PROGRESS: at 46.20% examples, 287995 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:49:35: EPOCH 5 - PROGRESS: at 53.13% examples, 288203 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:49:45: EPOCH 5 - PROGRESS: at 61.08% examples, 289963 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:49:55: EPOCH 5 - PROGRESS: at 68.68% examples, 290083 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:50:05: EPOCH 5 - PROGRESS: at 75.45% examples, 289926 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:50:15: EPOCH 5 - PROGRESS: at 80.55% examples, 289348 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:50:25: EPOCH 5 - PROGRESS: at 86.52% examples, 289248 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:50:35: EPOCH 5 - PROGRESS: at 92.61% examples, 288850 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:50:45: EPOCH 5 - PROGRESS: at 99.25% examples, 288469 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:50:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 15:50:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 15:50:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 15:50:46: EPOCH - 5 : training on 51151273 raw words (43948251 effective words) took 152.4s, 288424 effective words/s\n",
      "INFO - 15:50:47: EPOCH 6 - PROGRESS: at 0.59% examples, 265558 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:50:57: EPOCH 6 - PROGRESS: at 6.46% examples, 275532 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:51:07: EPOCH 6 - PROGRESS: at 12.94% examples, 279624 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:51:17: EPOCH 6 - PROGRESS: at 19.62% examples, 283590 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:51:27: EPOCH 6 - PROGRESS: at 25.81% examples, 280923 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:51:37: EPOCH 6 - PROGRESS: at 31.40% examples, 274234 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:51:47: EPOCH 6 - PROGRESS: at 37.46% examples, 272633 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:51:57: EPOCH 6 - PROGRESS: at 43.60% examples, 272054 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:52:07: EPOCH 6 - PROGRESS: at 50.33% examples, 274804 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:52:17: EPOCH 6 - PROGRESS: at 58.23% examples, 278065 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:52:27: EPOCH 6 - PROGRESS: at 65.99% examples, 279965 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:52:37: EPOCH 6 - PROGRESS: at 73.48% examples, 280680 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:52:47: EPOCH 6 - PROGRESS: at 78.39% examples, 279505 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:52:57: EPOCH 6 - PROGRESS: at 83.36% examples, 278471 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:53:07: EPOCH 6 - PROGRESS: at 88.82% examples, 277251 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:53:17: EPOCH 6 - PROGRESS: at 94.96% examples, 276473 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:53:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 15:53:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 15:53:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 15:53:26: EPOCH - 6 : training on 51151273 raw words (43950281 effective words) took 159.8s, 274992 effective words/s\n",
      "INFO - 15:53:27: EPOCH 7 - PROGRESS: at 0.53% examples, 224875 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:53:37: EPOCH 7 - PROGRESS: at 5.26% examples, 226508 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:53:47: EPOCH 7 - PROGRESS: at 10.86% examples, 235360 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:53:57: EPOCH 7 - PROGRESS: at 17.38% examples, 251160 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:54:07: EPOCH 7 - PROGRESS: at 23.62% examples, 257082 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:54:17: EPOCH 7 - PROGRESS: at 29.91% examples, 261155 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:54:27: EPOCH 7 - PROGRESS: at 35.92% examples, 261476 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:54:37: EPOCH 7 - PROGRESS: at 41.74% examples, 260321 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:54:47: EPOCH 7 - PROGRESS: at 48.12% examples, 262676 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:54:57: EPOCH 7 - PROGRESS: at 54.87% examples, 263544 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:55:07: EPOCH 7 - PROGRESS: at 62.29% examples, 265515 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:55:17: EPOCH 7 - PROGRESS: at 68.66% examples, 263609 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:55:27: EPOCH 7 - PROGRESS: at 74.17% examples, 259852 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:55:37: EPOCH 7 - PROGRESS: at 78.94% examples, 260203 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:55:47: EPOCH 7 - PROGRESS: at 84.31% examples, 260958 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:55:57: EPOCH 7 - PROGRESS: at 90.28% examples, 262761 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:56:07: EPOCH 7 - PROGRESS: at 96.64% examples, 263575 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:56:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 15:56:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 15:56:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 15:56:12: EPOCH - 7 : training on 51151273 raw words (43949351 effective words) took 166.5s, 263923 effective words/s\n",
      "INFO - 15:56:13: EPOCH 8 - PROGRESS: at 0.62% examples, 282566 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:56:23: EPOCH 8 - PROGRESS: at 6.24% examples, 266829 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:56:33: EPOCH 8 - PROGRESS: at 11.71% examples, 254084 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:56:43: EPOCH 8 - PROGRESS: at 17.71% examples, 257043 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:56:53: EPOCH 8 - PROGRESS: at 24.23% examples, 264459 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:57:03: EPOCH 8 - PROGRESS: at 30.10% examples, 263407 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:57:13: EPOCH 8 - PROGRESS: at 36.63% examples, 267167 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:57:23: EPOCH 8 - PROGRESS: at 43.14% examples, 269625 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:57:33: EPOCH 8 - PROGRESS: at 49.37% examples, 270029 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:57:43: EPOCH 8 - PROGRESS: at 56.49% examples, 271175 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:57:53: EPOCH 8 - PROGRESS: at 63.06% examples, 269151 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:58:03: EPOCH 8 - PROGRESS: at 70.40% examples, 270268 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:58:13: EPOCH 8 - PROGRESS: at 76.36% examples, 270386 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 15:58:23: EPOCH 8 - PROGRESS: at 81.38% examples, 270969 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:58:33: EPOCH 8 - PROGRESS: at 87.46% examples, 272080 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:58:43: EPOCH 8 - PROGRESS: at 93.50% examples, 272431 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:58:53: EPOCH 8 - PROGRESS: at 99.69% examples, 271710 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:58:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 15:58:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 15:58:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 15:58:54: EPOCH - 8 : training on 51151273 raw words (43949543 effective words) took 161.7s, 271754 effective words/s\n",
      "INFO - 15:58:55: EPOCH 9 - PROGRESS: at 0.59% examples, 255917 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:59:05: EPOCH 9 - PROGRESS: at 5.41% examples, 233106 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:59:15: EPOCH 9 - PROGRESS: at 11.10% examples, 240922 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 15:59:25: EPOCH 9 - PROGRESS: at 17.25% examples, 249860 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:59:35: EPOCH 9 - PROGRESS: at 22.91% examples, 249651 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:59:45: EPOCH 9 - PROGRESS: at 29.02% examples, 253675 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 15:59:55: EPOCH 9 - PROGRESS: at 34.90% examples, 254399 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:00:05: EPOCH 9 - PROGRESS: at 40.99% examples, 256012 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:00:15: EPOCH 9 - PROGRESS: at 46.79% examples, 255534 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:00:25: EPOCH 9 - PROGRESS: at 52.90% examples, 255560 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:00:35: EPOCH 9 - PROGRESS: at 59.87% examples, 256642 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:00:45: EPOCH 9 - PROGRESS: at 67.00% examples, 258214 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:00:55: EPOCH 9 - PROGRESS: at 74.00% examples, 259248 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:01:05: EPOCH 9 - PROGRESS: at 78.35% examples, 257836 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:01:15: EPOCH 9 - PROGRESS: at 83.53% examples, 259008 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:01:25: EPOCH 9 - PROGRESS: at 88.86% examples, 258855 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:01:35: EPOCH 9 - PROGRESS: at 94.61% examples, 258160 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:01:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:01:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:01:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:01:44: EPOCH - 9 : training on 51151273 raw words (43950030 effective words) took 170.6s, 257589 effective words/s\n",
      "INFO - 16:01:45: EPOCH 10 - PROGRESS: at 0.39% examples, 168160 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:01:55: EPOCH 10 - PROGRESS: at 5.63% examples, 242256 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:02:06: EPOCH 10 - PROGRESS: at 12.02% examples, 260052 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:02:16: EPOCH 10 - PROGRESS: at 18.24% examples, 264077 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:02:26: EPOCH 10 - PROGRESS: at 22.89% examples, 249823 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:02:36: EPOCH 10 - PROGRESS: at 28.11% examples, 246051 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:02:46: EPOCH 10 - PROGRESS: at 33.67% examples, 245697 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:02:56: EPOCH 10 - PROGRESS: at 39.25% examples, 245424 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:03:06: EPOCH 10 - PROGRESS: at 43.60% examples, 238447 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:03:16: EPOCH 10 - PROGRESS: at 48.00% examples, 233387 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:03:26: EPOCH 10 - PROGRESS: at 53.57% examples, 232701 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:03:36: EPOCH 10 - PROGRESS: at 59.89% examples, 233565 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:03:46: EPOCH 10 - PROGRESS: at 66.89% examples, 236354 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:03:56: EPOCH 10 - PROGRESS: at 74.24% examples, 240377 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:04:06: EPOCH 10 - PROGRESS: at 79.20% examples, 242798 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:04:16: EPOCH 10 - PROGRESS: at 84.11% examples, 243309 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 16:04:26: EPOCH 10 - PROGRESS: at 89.52% examples, 244502 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:04:36: EPOCH 10 - PROGRESS: at 95.62% examples, 245695 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 16:04:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:04:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:04:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:04:43: EPOCH - 10 : training on 51151273 raw words (43947095 effective words) took 178.2s, 246662 effective words/s\n",
      "INFO - 16:04:43: training on a 511512730 raw words (439489181 effective words) took 1649.3s, 266473 effective words/s\n",
      "INFO - 16:04:47: saving FastText object under embeddings.txt, separately None\n",
      "INFO - 16:04:47: storing np array 'vectors_ngrams' to embeddings.txt.wv.vectors_ngrams.npy\n",
      "INFO - 16:04:49: not storing attribute vectors_norm\n",
      "INFO - 16:04:49: not storing attribute vectors_vocab_norm\n",
      "INFO - 16:04:49: not storing attribute vectors_ngrams_norm\n",
      "INFO - 16:04:49: not storing attribute buckets_word\n",
      "INFO - 16:04:49: storing np array 'vectors_ngrams_lockf' to embeddings.txt.trainables.vectors_ngrams_lockf.npy\n",
      "INFO - 16:05:10: saved embeddings.txt\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = train_fasttext(tokenized_text=tokens, size=100, window=5, min_count=20, epochs=10, random_seed=123,\n",
    "                                vec_file_path=os.path.join('embeddings.txt'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# fasttext model's vocab size\n",
    "print(f'fasttext model\\'s vocabulary size: {len(fasttext_model.wv.vocab):,}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext model's vocabulary size: 72,827\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate embeddings\n",
    "Here we evaluate the embeddings learned by just ðŸ‘€ at the neighbours of a few words and examining if they are similar.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:05:10: precomputing L2-norms of word weight vectors\n",
      "INFO - 16:05:10: precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('à¬—à¬›à¬¡à¬¾à¬³', 0.8653923273086548),\n ('à¬¬à¬°à¬—à¬›', 0.8522686958312988),\n ('à¬—à¬›à¬°', 0.8209975957870483),\n ('à¬—à¬›à¬Ÿà¬¿', 0.801505982875824),\n ('à¬†à¬®à­à¬¬à¬—à¬›', 0.7687293291091919),\n ('à¬—à¬›à¬Ÿà¬¿à¬', 0.7585610747337341),\n ('à¬•à¬¦à¬³à­€à¬—à¬›', 0.7574436664581299),\n ('à¬«à­à¬²à¬—à¬›', 0.753203809261322),\n ('à¬—à¬›à¬•à¬¾à¬Ÿà¬¿', 0.7440799474716187),\n ('à¬¶à¬¾à¬³à¬—à¬›', 0.7383095622062683)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## find words similar to a given word\n",
    "fasttext_model.wv.most_similar('à¬—à¬›', topn=10)  # TODO: misspell it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[('à¬¸à¬‚à¬—à­€à¬¤', 0.9609819054603577),\n ('à¬¸à¬‚à¬™à­à¬—à­€à¬¤', 0.9506467580795288),\n ('à¬¸à¬™à­à¬—à­€à¬¤à¬°', 0.9171082973480225),\n ('à¬¸à¬™à­à¬—à­€à¬¤à¬•à¬¾à¬°', 0.893383264541626),\n ('à¬¸à¬‚à¬—à­€à¬¤à¬•à¬¾à¬°', 0.8804852366447449),\n ('à¬¸à¬‚à¬—à­€à¬¤à¬°', 0.8760529160499573),\n ('à¬¸à¬™à­à¬—à­€à¬¤à¬œà­à¬ž', 0.8728147745132446),\n ('à¬—à­€à¬¤à¬¿à¬¨à¬¾à¬Ÿà­à­Ÿ', 0.8677797913551331),\n ('à¬¨à­ƒà¬¤à­à­Ÿà¬—à­€à¬¤', 0.8639868497848511),\n ('à¬¸à¬‚à¬—à­€à¬¤à¬œà­à¬ž', 0.8267605304718018)]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar('à¬¸à¬™à­à¬—à­€à¬¤', topn=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[('à¬šà¬³à¬šà¬¿à¬¤à­à¬°', 0.930408239364624),\n ('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°à¬Ÿà¬¿', 0.9154518246650696),\n ('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°à¬Ÿà¬¿à¬°', 0.9073564410209656),\n ('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°à¬°', 0.8907100558280945),\n ('à¬¸à¬¿à¬¨à­‡à¬®à¬¾', 0.833586573600769),\n ('à¬¸à¬¿à¬¨à­‡à¬®à¬¾à¬Ÿà­‹à¬—à­à¬°à¬¾à¬«à¬°', 0.8292860388755798),\n ('à¬«à¬¿à¬²à­à¬®', 0.826623797416687),\n ('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°à¬°à­‡', 0.8206254243850708),\n ('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°à¬Ÿà¬¿à¬°à­‡', 0.8183248043060303),\n ('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°à¬•à­', 0.8024610280990601)]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°', topn=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Try some misspelled words**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°', 0.8048502206802368),\n ('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°à¬Ÿà¬¿à¬°', 0.7769005298614502),\n ('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°à¬Ÿà¬¿', 0.7636350393295288),\n ('à¬¸à¬®à­à¬šà­à¬šà¬¿à¬¤', 0.7427504658699036),\n ('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°à¬°', 0.7388564348220825),\n ('à¬²à­‹à¬•à¬ªà­à¬°à¬¿à­Ÿ', 0.7327723503112793),\n ('à¬¸à¬¨à­à¬¨à¬¿à¬¬à­‡à¬¶à¬¿à¬¤', 0.7062867879867554),\n ('à¬«à¬¿à¬²à­à¬®', 0.7052443027496338),\n ('à¬šà¬³à¬šà­à¬šà¬¿à¬¤à­à¬°à¬Ÿà¬¿à¬°à­‡', 0.7045912742614746),\n ('à¬¸à¬¿à¬¨à­‡à¬®à¬¾à¬Ÿà­‹à¬—à­à¬°à¬¾à¬«à¬°', 0.6947799921035767)]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar('à¬šà¬³à¬šà­à¬šà¬¿à¬¤', topn=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[('à¬—à­€à¬¤', 0.9333454370498657),\n ('à¬¸à¬‚à¬™à­à¬—à­€à¬¤', 0.8912193179130554),\n ('à¬¨à­ƒà¬¤à­à­Ÿà¬—à­€à¬¤', 0.8894705772399902),\n ('à¬¨à¬¾à¬šà¬—à­€à¬¤', 0.8886850476264954),\n ('à¬¸à¬‚à¬—à­€à¬¤', 0.8765240907669067),\n ('à¬¸à¬™à­à¬—à­€à¬¤', 0.8741494417190552),\n ('à¬—à­€à¬¤à­', 0.8588917255401611),\n ('à¬—à­€à¬¤à¬¿à¬¨à¬¾à¬Ÿà­à­Ÿ', 0.8219820261001587),\n ('à¬—à­€à¬¤à¬°', 0.7932537794113159),\n ('à¬¸à¬™à­à¬—à­€à¬¤à¬°', 0.7914323806762695)]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar('à¬¸à¬—à­€à¬¤', topn=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}